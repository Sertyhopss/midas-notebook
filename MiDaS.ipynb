{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "alpha-bathroom",
      "metadata": {
        "id": "alpha-bathroom"
      },
      "source": [
        "### This notebook is optionally accelerated with a GPU runtime.\n",
        "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "# MiDaS\n",
        "\n",
        "*Author: Intel ISL*\n",
        "\n",
        "**MiDaS models for computing relative depth from a single image.**\n",
        "\n",
        "<img src=\"https://pytorch.org/assets/images/midas_samples.png\" alt=\"alt\" width=\"50%\"/>\n",
        "\n",
        "\n",
        "### Model Description\n",
        "\n",
        "[MiDaS](https://arxiv.org/abs/1907.01341) computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using\n",
        "multi-objective optimization to ensure high quality on a wide range of inputs.\n",
        "\n",
        "### Dependencies\n",
        "\n",
        "MiDaS depends on [timm](https://github.com/rwightman/pytorch-image-models). Install with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "parental-toyota",
      "metadata": {
        "attributes": {
          "classes": [
            "shell"
          ],
          "id": ""
        },
        "id": "parental-toyota"
      },
      "outputs": [],
      "source": [
        "pip install timm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scientific-burning",
      "metadata": {
        "id": "scientific-burning"
      },
      "source": [
        "### Example Usage\n",
        "\n",
        "Download an image from the PyTorch homepage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "graphic-swing",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "graphic-swing",
        "outputId": "a69ecad8-d923-4d16-9a90-f08916c05f85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('dog.jpg', <http.client.HTTPMessage at 0x7fe476f04130>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import urllib.request\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\"\n",
        "filename = \"dog.jpg\"\n",
        "urllib.request.urlretrieve(url, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "central-spell",
      "metadata": {
        "id": "central-spell"
      },
      "source": [
        "Load a model (see [https://github.com/intel-isl/MiDaS/#Accuracy](https://github.com/intel-isl/MiDaS/#Accuracy) for an overview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brazilian-reach",
      "metadata": {
        "id": "brazilian-reach"
      },
      "outputs": [],
      "source": [
        "model_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n",
        "#model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\n",
        "#model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n",
        "\n",
        "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tired-delaware",
      "metadata": {
        "id": "tired-delaware"
      },
      "source": [
        "Move model to GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "assisted-count",
      "metadata": {
        "id": "assisted-count"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "midas.to(device)\n",
        "midas.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "extensive-transcript",
      "metadata": {
        "id": "extensive-transcript"
      },
      "source": [
        "Load transforms to resize and normalize the image for large or small model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "greenhouse-finder",
      "metadata": {
        "id": "greenhouse-finder"
      },
      "outputs": [],
      "source": [
        "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "\n",
        "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
        "    transform = midas_transforms.dpt_transform\n",
        "else:\n",
        "    transform = midas_transforms.small_transform"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tested-action",
      "metadata": {
        "id": "tested-action"
      },
      "source": [
        "Load image and apply transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fuzzy-grounds",
      "metadata": {
        "id": "fuzzy-grounds"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(filename)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "input_batch = transform(img).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "naughty-battle",
      "metadata": {
        "id": "naughty-battle"
      },
      "source": [
        "Predict and resize to original resolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "posted-quebec",
      "metadata": {
        "id": "posted-quebec"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    prediction = midas(input_batch)\n",
        "\n",
        "    prediction = torch.nn.functional.interpolate(\n",
        "        prediction.unsqueeze(1),\n",
        "        size=img.shape[:2],\n",
        "        mode=\"bicubic\",\n",
        "        align_corners=False,\n",
        "    ).squeeze()\n",
        "\n",
        "output = prediction.cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "incredible-campaign",
      "metadata": {
        "id": "incredible-campaign"
      },
      "source": [
        "Show result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vital-tribute",
      "metadata": {
        "id": "vital-tribute"
      },
      "outputs": [],
      "source": [
        "plt.imshow(output)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def write_depth(path, depth, bits=1):\n",
        "\n",
        "    depth_min = depth.min()\n",
        "    depth_max = depth.max()\n",
        "\n",
        "    max_val = 65535 # uint16 max value\n",
        "\n",
        "    if depth_max - depth_min > np.finfo(\"float\").eps:\n",
        "        out = max_val * (depth - depth_min) / (depth_max - depth_min)\n",
        "    else:\n",
        "        out = 0\n",
        "\n",
        "    cv2.imwrite(path + \".png\", out.astype(\"uint16\"))\n",
        "    \n",
        "    return"
      ],
      "metadata": {
        "id": "2d4k1OR_MYqC"
      },
      "id": "2d4k1OR_MYqC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "write_depth('/content/drive/MyDrive/dog.png', output)"
      ],
      "metadata": {
        "id": "y6u-E4XiGBtG"
      },
      "id": "y6u-E4XiGBtG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "expanded-verification",
      "metadata": {
        "id": "expanded-verification"
      },
      "source": [
        "### References\n",
        "[Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer](https://arxiv.org/abs/1907.01341)\n",
        "\n",
        "[Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413)\n",
        "\n",
        "Please cite our papers if you use our models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "under-refund",
      "metadata": {
        "attributes": {
          "classes": [
            "bibtex"
          ],
          "id": ""
        },
        "id": "under-refund"
      },
      "outputs": [],
      "source": [
        "@article{Ranftl2020,\n",
        "\tauthor    = {Ren\\'{e} Ranftl and Katrin Lasinger and David Hafner and Konrad Schindler and Vladlen Koltun},\n",
        "\ttitle     = {Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer},\n",
        "\tjournal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},\n",
        "\tyear      = {2020},\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beautiful-today",
      "metadata": {
        "attributes": {
          "classes": [
            "bibtex"
          ],
          "id": ""
        },
        "id": "beautiful-today"
      },
      "outputs": [],
      "source": [
        "@article{Ranftl2021,\n",
        "\tauthor    = {Ren\\'{e} Ranftl and Alexey Bochkovskiy and Vladlen Koltun},\n",
        "\ttitle     = {Vision Transformers for Dense Prediction},\n",
        "\tjournal   = {ArXiv preprint},\n",
        "\tyear      = {2021},\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}